{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì¼ í™•ì¸\n",
    "\n",
    "import os\n",
    "\n",
    "dataset_dir = \"/content/drive/MyDrive/Colab Notebooks/Training/ResNet_DOG\"\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print(\"âŒ ë°ì´í„°ì…‹ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"âœ… ë°ì´í„°ì…‹ í´ë” í™•ì¸ ì™„ë£Œ!\")\n",
    "    print(\"í´ë˜ìŠ¤ í´ë”:\", os.listdir(dataset_dir))\n",
    "\n",
    "\n",
    "for class_name in os.listdir(dataset_dir):\n",
    "    class_path = os.path.join(dataset_dir, class_name)\n",
    "    if os.path.isdir(class_path):  # í´ë˜ìŠ¤ í´ë”ì¸ì§€ í™•ì¸\n",
    "        print(f\"ğŸ”¹ {class_name} í´ë” ë‚´ ì´ë¯¸ì§€ ê°œìˆ˜: {len(os.listdir(class_path))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models\n",
    "import os\n",
    "\n",
    "# Google Drive ì—°ë™ (í•„ìš”í•œ ê²½ìš°)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def train_and_test():\n",
    "    # ------------------------\n",
    "    # 1. ê²½ë¡œ ë° ë§¤ê°œë³€ìˆ˜ ì„¤ì •\n",
    "    # ------------------------\n",
    "    dataset_dir = \"/content/drive/MyDrive/Colab Notebooks/Training/ResNet_DOG\"  # ì½”ë© ë¡œì»¬ íŒŒì¼ ê²½ë¡œ\n",
    "    batch_size = 16   # 16, 32, 64\n",
    "    num_epochs = 30   # 5, 10, 20, 30, 50\n",
    "    learning_rate = 0.0001   # 0.1, 0,01, 0.001(1e-3), 0.0005, 0.0001(1e-4), 0.00001\n",
    "    log_file = \"/content/drive/MyDrive/Colab Notebooks/Training/training_log.txt\"  # ë¡œê·¸ íŒŒì¼ ì €ì¥ ê²½ë¡œ\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"Training Log\\n\")\n",
    "        f.write(f\"Batch Size: {batch_size}\\n\")\n",
    "        f.write(f\"Epochs: {num_epochs}\\n\")\n",
    "        f.write(f\"Learning Rate: {learning_rate}\\n\")\n",
    "        f.write(f\"Device: {device}\\n\\n\")\n",
    "\n",
    "    # ------------------------\n",
    "    # 2. ë°ì´í„° ë³€í™˜ ì„¤ì •\n",
    "    # ------------------------\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # ------------------------\n",
    "    # 3. ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„í• \n",
    "    # ------------------------\n",
    "    full_dataset = torchvision.datasets.ImageFolder(root=dataset_dir, transform=transform)\n",
    "    total_images = len(full_dataset)\n",
    "\n",
    "    train_size = int(0.7 * total_images)\n",
    "    val_size = int(0.15 * total_images)\n",
    "    test_size = total_images - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    print(f\"Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"Train Samples: {len(train_dataset)}\\n\")\n",
    "        f.write(f\"Validation Samples: {len(val_dataset)}\\n\")\n",
    "        f.write(f\"Test Samples: {len(test_dataset)}\\n\\n\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    print(\"Classes:\", full_dataset.classes)\n",
    "\n",
    "    # ------------------------\n",
    "    # 4. ResNet ëª¨ë¸ ì •ì˜ (ê°„ëµí•œ ëª¨ë¸ ì •ë³´ ë¡œê·¸)\n",
    "    # ------------------------\n",
    "    model_name = \"ResNet-18\"  # ë³€ê²½ ê°€ëŠ¥ (ì˜ˆ: \"ResNet-50\" ë“±)\n",
    "    model = models.resnet18(weights=\"IMAGENET1K_V1\")  # ResNet-50 ì‚¬ìš© ì‹œ models.resnet50()\n",
    "\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # ------------------------\n",
    "    # 5. ì†ì‹¤ ë° ì˜µí‹°ë§ˆì´ì €\n",
    "    # ------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    weight_decay = 1e-4\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)   # AdamW ì ìš©\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"Criterion: {criterion}\\n\")\n",
    "        f.write(f\"Optimizer: {optimizer}\\n\\n\")\n",
    "\n",
    "    # ------------------------\n",
    "    # 6. í›ˆë ¨ ë£¨í”„\n",
    "    # ------------------------\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        log_msg = (f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                   f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "                   f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "        # ë¡œê·¸ íŒŒì¼ ì €ì¥\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(log_msg + \"\\n\")\n",
    "\n",
    "    # ------------------------\n",
    "    # 7. í…ŒìŠ¤íŠ¸\n",
    "    # ------------------------\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "    test_acc = 100.0 * test_correct / test_total\n",
    "    test_msg = f\"Test Accuracy: {test_acc:.2f}%\"\n",
    "    print(test_msg)\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê·¸ ì €ì¥\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"\\n\" + test_msg + \"\\n\")\n",
    "\n",
    "    # ------------------------\n",
    "    # 8. ëª¨ë¸ ì €ì¥\n",
    "    # ------------------------\n",
    "    model_save_path = \"/content/drive/MyDrive/Colab Notebooks/Training/resnet18_balanced_model.pth\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"\\nModel saved at: {model_save_path}\\n\")\n",
    "        f.write(\"Training and testing complete!\\n\")\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna (ìµœì ì˜ íŒŒë¼ë¯¸í„° ì°¾ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models\n",
    "import os\n",
    "import optuna  # Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Google Drive ì—°ë™ (í•„ìš”í•œ ê²½ìš°)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 1. ë°ì´í„°ì…‹ ì„¤ì •\n",
    "# ------------------------\n",
    "dataset_dir = \"/content/drive/MyDrive/Colab Notebooks/Training/ResNet_DOG\"  # í´ë” ì•ˆì— í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ì¡´ì¬\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")   # colab\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"âœ… Using device:\", device)\n",
    "\n",
    "\n",
    "def train_model(trial):\n",
    "    \"\"\"\n",
    "    Optunaê°€ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê¸° ìœ„í•œ ëª©ì  í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) ìµœì ì˜ Batch Size ì°¾ê¸°\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])\n",
    "\n",
    "    # ë°ì´í„° ë¡œë” ìƒì„± (num_workers=8, pin_memory=True ì„¤ì •)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    # 2) ResNet50 ëª¨ë¸ ìƒì„±\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # 3) ìµœì ì˜ Optimizer ì°¾ê¸° (Adam vs AdamW vs SGD)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"SGD\"])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # ìµœì ì˜ learning rate íƒìƒ‰\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2)  # ìµœì ì˜ weight decay íƒìƒ‰\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # í˜¼í•© ì •ë°€ë„ í•™ìŠµ (FP16 í™œìš©)\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    # 4) ëª¨ë¸ í•™ìŠµ (5 Epochë§Œ ì‹¤í–‰, Optunaì—ì„œ ë¹ ë¥¸ íƒìƒ‰ì„ ìœ„í•´)\n",
    "    num_epochs = 15\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\"):  # Mixed Precision Training     # mps\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # 5) ê²€ì¦ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ í‰ê°€\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with torch.autocast(device_type=\"cuda\"):    # mps \n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    return val_loss  # ê²€ì¦ ë°ì´í„° ì†ì‹¤ê°’ì´ ìµœì†Œê°€ ë˜ëŠ” ì¡°í•©ì„ ì°¾ìŒ\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 6. ë©”ì¸ ì‹¤í–‰ (ë©€í‹°í”„ë¡œì„¸ì‹± í•´ê²°)\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„í• \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    full_dataset = torchvision.datasets.ImageFolder(root=dataset_dir, transform=transform)\n",
    "    total_images = len(full_dataset)\n",
    "\n",
    "    # Train/Validation/Test Split (70:15:15)\n",
    "    train_size = int(0.7 * total_images)\n",
    "    val_size = int(0.15 * total_images)\n",
    "    test_size = total_images - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    print(f\"ğŸ”¹ Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    print(\"ğŸ”¹ Classes:\", full_dataset.classes)\n",
    "\n",
    "    # Optuna ì‹¤í–‰ (20íšŒ íƒìƒ‰)\n",
    "    study = optuna.create_study(direction=\"minimize\")  # ìµœì†Œì˜ val_lossë¥¼ ì°¾ëŠ” ë°©í–¥\n",
    "    study.optimize(train_model, n_trials=20)  # 20íšŒ íƒìƒ‰\n",
    "\n",
    "    # ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥\n",
    "    best_params = study.best_params\n",
    "    print(\"\\nâœ… ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì°¾ê¸° ì™„ë£Œ!\")\n",
    "    print(best_params)\n",
    "\n",
    "    # ------------------------\n",
    "    # 7. ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì¬í›ˆë ¨\n",
    "    # ------------------------\n",
    "    batch_size = best_params[\"batch_size\"]\n",
    "    learning_rate = best_params[\"learning_rate\"]\n",
    "    weight_decay = best_params[\"weight_decay\"]\n",
    "    optimizer_name = best_params[\"optimizer\"]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    print(\"ğŸš€ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ì„ ì¬í•™ìŠµí•˜ì„¸ìš”!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
