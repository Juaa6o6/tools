{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 확인\n",
    "\n",
    "import os\n",
    "\n",
    "dataset_dir = \"/content/drive/MyDrive/Colab Notebooks/Training/ResNet_DOG\"\n",
    "\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print(\"❌ 데이터셋 폴더가 존재하지 않습니다!\")\n",
    "else:\n",
    "    print(\"✅ 데이터셋 폴더 확인 완료!\")\n",
    "    print(\"클래스 폴더:\", os.listdir(dataset_dir))\n",
    "\n",
    "\n",
    "for class_name in os.listdir(dataset_dir):\n",
    "    class_path = os.path.join(dataset_dir, class_name)\n",
    "    if os.path.isdir(class_path):  # 클래스 폴더인지 확인\n",
    "        print(f\"🔹 {class_name} 폴더 내 이미지 개수: {len(os.listdir(class_path))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models\n",
    "import os\n",
    "\n",
    "# Google Drive 연동 (필요한 경우)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def train_and_test():\n",
    "    # ------------------------\n",
    "    # 1. 경로 및 매개변수 설정\n",
    "    # ------------------------\n",
    "    dataset_dir = \"/content/drive/MyDrive/Colab Notebooks/Training/ResNet_DOG\"  # 코랩 로컬 파일 경로\n",
    "    batch_size = 16   # 16, 32, 64\n",
    "    num_epochs = 30   # 5, 10, 20, 30, 50\n",
    "    learning_rate = 0.0001   # 0.1, 0,01, 0.001(1e-3), 0.0005, 0.0001(1e-4), 0.00001\n",
    "    log_file = \"/content/drive/MyDrive/Colab Notebooks/Training/training_log.txt\"  # 로그 파일 저장 경로\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # 로그 파일 초기화\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"Training Log\\n\")\n",
    "        f.write(f\"Batch Size: {batch_size}\\n\")\n",
    "        f.write(f\"Epochs: {num_epochs}\\n\")\n",
    "        f.write(f\"Learning Rate: {learning_rate}\\n\")\n",
    "        f.write(f\"Device: {device}\\n\\n\")\n",
    "\n",
    "    # ------------------------\n",
    "    # 2. 데이터 변환 설정\n",
    "    # ------------------------\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # ------------------------\n",
    "    # 3. 데이터셋 로드 및 분할\n",
    "    # ------------------------\n",
    "    full_dataset = torchvision.datasets.ImageFolder(root=dataset_dir, transform=transform)\n",
    "    total_images = len(full_dataset)\n",
    "\n",
    "    train_size = int(0.7 * total_images)\n",
    "    val_size = int(0.15 * total_images)\n",
    "    test_size = total_images - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    print(f\"Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"Train Samples: {len(train_dataset)}\\n\")\n",
    "        f.write(f\"Validation Samples: {len(val_dataset)}\\n\")\n",
    "        f.write(f\"Test Samples: {len(test_dataset)}\\n\\n\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    print(\"Classes:\", full_dataset.classes)\n",
    "\n",
    "    # ------------------------\n",
    "    # 4. ResNet 모델 정의 (간략한 모델 정보 로그)\n",
    "    # ------------------------\n",
    "    model_name = \"ResNet-18\"  # 변경 가능 (예: \"ResNet-50\" 등)\n",
    "    model = models.resnet18(weights=\"IMAGENET1K_V1\")  # ResNet-50 사용 시 models.resnet50()\n",
    "\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # ------------------------\n",
    "    # 5. 손실 및 옵티마이저\n",
    "    # ------------------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    weight_decay = 1e-4\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)   # AdamW 적용\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"Criterion: {criterion}\\n\")\n",
    "        f.write(f\"Optimizer: {optimizer}\\n\\n\")\n",
    "\n",
    "    # ------------------------\n",
    "    # 6. 훈련 루프\n",
    "    # ------------------------\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        log_msg = (f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                   f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "                   f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        print(log_msg)\n",
    "\n",
    "        # 로그 파일 저장\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(log_msg + \"\\n\")\n",
    "\n",
    "    # ------------------------\n",
    "    # 7. 테스트\n",
    "    # ------------------------\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "    test_acc = 100.0 * test_correct / test_total\n",
    "    test_msg = f\"Test Accuracy: {test_acc:.2f}%\"\n",
    "    print(test_msg)\n",
    "\n",
    "    # 테스트 결과 로그 저장\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\"\\n\" + test_msg + \"\\n\")\n",
    "\n",
    "    # ------------------------\n",
    "    # 8. 모델 저장\n",
    "    # ------------------------\n",
    "    model_save_path = \"/content/drive/MyDrive/Colab Notebooks/Training/resnet18_balanced_model.pth\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"\\nModel saved at: {model_save_path}\\n\")\n",
    "        f.write(\"Training and testing complete!\\n\")\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna (최적의 파라미터 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models\n",
    "import os\n",
    "import optuna  # Optuna 라이브러리\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Google Drive 연동 (필요한 경우)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 1. 데이터셋 설정\n",
    "# ------------------------\n",
    "dataset_dir = \"/content/drive/MyDrive/Colab Notebooks/Training/ResNet_DOG\"  # 폴더 안에 클래스별 이미지 존재\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")   # colab\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"✅ Using device:\", device)\n",
    "\n",
    "\n",
    "def train_model(trial):\n",
    "    \"\"\"\n",
    "    Optuna가 최적의 하이퍼파라미터를 찾기 위한 목적 함수\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) 최적의 Batch Size 찾기\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])\n",
    "\n",
    "    # 데이터 로더 생성 (num_workers=8, pin_memory=True 설정)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n",
    "\n",
    "    # 2) ResNet50 모델 생성\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # 3) 최적의 Optimizer 찾기 (Adam vs AdamW vs SGD)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"SGD\"])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)  # 최적의 learning rate 탐색\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-2)  # 최적의 weight decay 탐색\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    # 손실 함수 정의\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 혼합 정밀도 학습 (FP16 활용)\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    # 4) 모델 학습 (5 Epoch만 실행, Optuna에서 빠른 탐색을 위해)\n",
    "    num_epochs = 15\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\"):  # Mixed Precision Training     # mps\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # 5) 검증 데이터에서 성능 평가\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with torch.autocast(device_type=\"cuda\"):    # mps \n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    return val_loss  # 검증 데이터 손실값이 최소가 되는 조합을 찾음\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 6. 메인 실행 (멀티프로세싱 해결)\n",
    "# ------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 데이터셋 로드 및 분할\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    full_dataset = torchvision.datasets.ImageFolder(root=dataset_dir, transform=transform)\n",
    "    total_images = len(full_dataset)\n",
    "\n",
    "    # Train/Validation/Test Split (70:15:15)\n",
    "    train_size = int(0.7 * total_images)\n",
    "    val_size = int(0.15 * total_images)\n",
    "    test_size = total_images - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    num_classes = len(full_dataset.classes)\n",
    "    print(f\"🔹 Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "    print(\"🔹 Classes:\", full_dataset.classes)\n",
    "\n",
    "    # Optuna 실행 (20회 탐색)\n",
    "    study = optuna.create_study(direction=\"minimize\")  # 최소의 val_loss를 찾는 방향\n",
    "    study.optimize(train_model, n_trials=20)  # 20회 탐색\n",
    "\n",
    "    # 최적의 하이퍼파라미터 출력\n",
    "    best_params = study.best_params\n",
    "    print(\"\\n✅ 최적의 하이퍼파라미터 찾기 완료!\")\n",
    "    print(best_params)\n",
    "\n",
    "    # ------------------------\n",
    "    # 7. 최적의 하이퍼파라미터를 사용하여 모델 재훈련\n",
    "    # ------------------------\n",
    "    batch_size = best_params[\"batch_size\"]\n",
    "    learning_rate = best_params[\"learning_rate\"]\n",
    "    weight_decay = best_params[\"weight_decay\"]\n",
    "    optimizer_name = best_params[\"optimizer\"]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    print(\"🚀 최적의 하이퍼파라미터로 모델을 재학습하세요!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
